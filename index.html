<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Donghuo Zeng - Homepage</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background: linear-gradient(to right, #f9f9f9, #e3e3e3);
            color: #333;
        }
        .avatar {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            box-shadow: 0 5px 8px rgba(0,0,0,0.2);
        }
        .container {
            max-width: 900px;
            margin: auto;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        }
        .hr-container {
            position: relative;
            text-align: center;
            margin: 30px 0;
            width:125%
        }
        .hr-container hr {
            border: none;
            border-top: 6px solid #007aff; /* Apple-style blue */
            width: 100%;
        }
        .hr-container .title {
            position: absolute;
            top: -20px;
            left: 45%;
            transform: translateX(-50%);
            background-color: white;
            padding: 0 15px;
            font-size: 1.5rem;
            font-weight: bold;
            white-space: nowrap; /* Prevents wrapping */;
        }
        .contact-icons a {
            color: #007aff;
            margin-right: 10px;
            font-size: 1.2rem;
            transition: color 0.3s ease;
        }
        .contact-icons a:hover {
            color: #0056b3;
        }
        .section-title {
            color: #007aff;
            font-weight: bold;
        }
        .card {
            border: none;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            transition: transform 0.2s ease;
        }
        .card:hover {
            transform: scale(1.02);
        }
    </style>
</head>
<body>
    <div class="container mt-4 p-4">
        <div class="text-center">
            <img src="images/donghuo.png" alt="Donghuo Zeng" class="avatar">
            <div class="hr-container">
                <hr />
                <div class="title">Donghuo Zeng</div>
            </div>
            <h4 class="text-muted">AI Researcher at KDDI Research, Inc., Japan</h4>
            <p>Specializing in multimedia learning, persuasion systems, and causality.</p>
        </div>

        <!-- Contact Section -->
        <p>
            <strong>Email:</strong> <a href="mailto:do-zeng@kddi-research.jp">do-zeng@kddi-research.jp</a><br>
            <div class="contact-icons">
                <a href="https://twitter.com/dhjulianzeng" target="_blank"><i class="fab fa-twitter"></i></a>
                <a href="https://www.linkedin.com/in/donghuo-zeng-205a14223/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="https://scholar.google.com/citations?user=S2tvWCUAAAAJ&hl=en&authuser=2" target="_blank"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://www.researchgate.net/profile/Donghuo-Zeng/research" target="_blank"><i class="fab fa-researchgate"></i></a>
            </div>
            <strong>Address:</strong> 〒356-0003 Saitama, Fujimino, Ohara, 2 Chome−1−15 KDDI Research, Inc.
        </p>

        <!-- Professional Summary -->
        <h2 class="section-title">Professional Summary</h2>
        <div class="card p-3">
            <p>AI researcher at KDDI Research, Inc., Japan, specializing in multimedia learning, persuasion systems, and causality. Passionate about developing innovative AI methodologies to enhance
            human-computer interaction and decision-making processes. Academic background with a Ph.D. from the National Institute of Informatics (NII), Japan.</p>
            <ul>
                <li><strong>2021 - Present:</strong> AI Researcher at KDDI Research, Inc.</li>
                <li><strong>2023 - 2024:</strong> Collaboration with Carnegie Mellon University (CMU)</li>
                <li><strong>2020 - 2021:</strong> Research Assistant at NII, Japan</li>
                <li><strong>2017 - 2020:</strong> Ph.D. from National Institute of Informatics (NII), Japan</li>
            </ul>
        </div>

        <!-- Activity -->
        <h2 class="section-title mt-4">Activity</h2>
        <ul class="list-group">
            <li class="list-group-item"><strong>2024.6:</strong> Presented at ACM ICMR 2024 in Phuket, Thailand.</li>
            <li class="list-group-item"><strong>2023.7:</strong> Presented at JSAI 2023 in Kumamoto, Japan.</li> 
            <li class="list-group-item"><strong>2022.12:</strong> Presented at IEEE ISM 2022 online (Naples, Italy).</li>
            <li class="list-group-item"><strong>2021.12:</strong> Presented at IEEE ISM 2021 online (Naples, Italy).</li>
            <li class="list-group-item"><strong>2021.12:</strong> Presented at ACM MUM 2021 online (Leuven, Belgium).</li> 
            <li class="list-group-item"><strong>2020.11:</strong> Presented at CSMT 2020 online (Taiyuan, Shanxi, China).</li> 
            <li class="list-group-item"><strong>2020.9:</strong> Presented at the IEEE BigMM 2020 conference online (New Delhi, India).</li>
            <li class="list-group-item"><strong>2019.11:</strong> Presented at the ICDM 2019 conference in Beijing, China.</li>
            <li class="list-group-item"><strong>2019.3:</strong> Presented at DEIM 2019 conference in Nagasaki, Japan.</li>
            <li class="list-group-item"><strong>2018.12:</strong> Presented at IEEE ISM 2018 in Taichung, Taiwan.</li>
            <li class="list-group-item"><strong>2018.11:</strong> Research Visitor at University of Illinois Urbana-Champaign (UIUC), USA.</li> 
            <li class="list-group-item"><strong>2018.6:</strong> Attended (also Staff) ACM ICMR 2018 in Yokohama Japan.</li>
        </ul>

        <!-- Grand Challenges -->
        <h2 class="section-title mt-4">Grand Challenges</h2>
        <ul>
            <li><a href="https://sites.google.com/view/emotiw2023">Emotion Recognition in the Wild Challenge (EmotiW23)</a></li>
        </ul>

        <!-- Publications -->
        <h2 class="section-title mt-4">Publications</h2>
        <div class="card p-3">
            <ol
                 <li>Donghuo Zeng, R. Legaspi, Yunwen Sun, Xinshuai Dong, Kazushi Ikeda, P. Spirtes, K. Zhang, "Causal discovery and counterfactual reasoning to optimize persuasive dialogue policies", <b>Behaviour & Information Technology (BiT) 2025.</b> (Full, IF: 3.7, Acceptance rate: 9%) </li>
                <li>Donghuo Zeng, Kazushi Ikeda, "Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning", IEEE International Conference on Acoustics, Speech, and Signal Processing (<b>ICASSP 2025</b>), 2025.(Full oral presentation, H5: 139, Acceptance rate: ~43%) </li>
                <li>Donghuo Zeng, Yanan Wang, Kazushi Ikeda, Yi Yu, "Anchor-aware Deep Metric Learning for Audio-visual Retrieval", ACM International Conference on Multimedia Retrieval 2024 (<b>ICMR '24</b>), 2024. (Full oral presentation, Acceptance rate: 12.81%</li>
                <li>Yunwen Sun, B. Huang, Y. Yao, Donghuo Zeng, Xinshuai Dong, S. Jin, B. Sun, R. Legaspi, Kazushi Ikeda, "Identifying Latent State-Transition Processes for Individualized Reinforcement Learning", Advances in Neural Information Processing Systems (<b>NeurIPS</b>)37, 2024. (Full paper, Poster presentation, Acceptance rate: 25.8%)</li>
                <li>Yanan Wang, S. Haruta, Donghuo Zeng, J. Vizcarra, M. Kurokawa, "Multi-object event graph representation learning for Video Question Answering", <b>MIRU 2024</b>. (Full oral presentations).</li>
                <li>Yanan Wang, S. Haruta, Donghuo Zeng, J. Vizcarra, M. Kurokawa, "Top-down Activity Representation Learning for Video Question Answering", <b>MIRU2024</b>. (Full oral presentations)</li>
                <li>Donghuo Zeng, R. Legaspi, Yunwen Sun, Xinshuai Dong, Kazushi Ikeda, P. Spirtes, K. Zhang, "Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome", The 19th International Conference on <b>Persuasive Technology (PT 2024)</b>, 2024. (Full oral presentation, Acceptance rate: 27.5%)</li>
                <li>M. Singh, X. Hoque, Donghuo Zeng, Yanan Wang, Kazushi Ikeda, A. Dhall, "Do I have your attention: A large scale engagement prediction dataset and baselines", Proceedings of the 25th International Conference on Multimodal Interaction (<b>ICMI 2023</b>), 2023. (Full oral presentation, Acceptance rate: 37% (accepted as full)/10% (oral presentation))</li>
                <li>A. Dhall, M. Singh, R. Goecke, T. Gedeon, Donghuo Zeng, Yanan Wang, Kazushi Ikeda, "Emotiw 2023: Emotion recognition in the wild challenge", Proceedings of the 25th International Conference on Multimodal Interaction,(<b>ICMI 2023</b>) 2023. (Full, Grand Challenge)</li>
                <li>Donghuo Zeng, Jianming Wu, Gen Hattori, Rui Xu, Yi Yu, "Learning Explicit and Implicit Dual Common Subspaces for Audio-visual Cross-modal Retrieval", ACM Transactions on Multimedia Computing, Communications and Applications,(<b>ACM TOMM 2023</b>) 2023. (IF: 5.2)</li>
                <li>Yanan Wang, Donghuo Zeng, S. Wada, S. Kurihara, "Videoadviser: Video knowledge distillation for multimodal transfer learning", <b>IEEE Access, 2023</b>. (IF: 3.4)</li>
                <li>Donghuo Zeng, Kazushi Ikeda, "Triplet Loss with Curriculum Learning for Audio-Visual Retrieval", IEEE International Symposium on Multimedia (<b>ISM 2023</b>), 2023. (Position paper, Oral presentation, Acceptance rate: ~40%)</li>
                <li>Donghuo Zeng, Jianming Wu, Gen Hattori, Yoshihiko Takishima, "TV-watching partner robot: Analysis of User’s Experience", arXiv preprint arXiv:2302.14472, 2023.</li>
                <li>Donghuo Zeng, Jianming Wu, Yanan Wang, Kazuya Matsumoto, Gen Hattori, Keizo Ikeda, "Topic-switch adapted Japanese Dialogue System based on PLATO-2", Proceedings of the Annual Conference of JSAI (<b>JSAI 2023</b>), 2023.</li>
                <li>Donghuo Zeng, Yanan Wang, Jianming Wu, Kazushi Ikeda, "Complete cross-triplet loss in label space for audio-visual cross-modal retrieval", The 24th IEEE International Symposium on Multimedia (<b>ISM 2022</b>), 2022. (Full oral presentation, Acceptance rate: 22.22% (20/90))</li>
                <li>Donghuo Zeng, Jianming Wu, Baisheng Yang, Tomohiro Obara, Atsushi Okawa, Naoto Iino, Gen Hattori, Ryoji Kawada, "SHECS: A Local Smart Hands-free Elderly Care Support System on Smart AR Glasses with AI Technology", The 23rd IEEE International Symposium on Multimedia (<b>ISM 2021</b>), 2021. (Full oral presentation, Acceptance rate: 20/67 submissions (29.85%))</li>
                <li>Jianming Wu, Donghuo Zeng, Baisheng Yang, Hiroshi Gen, Yoshihiko Takishima, Yuji Hagio, Masahiro Kamimura, "TV-watching Companion Robot Supported by Open-domain Chatbot “KACTUS", Proceedings of the 20th International Conference on Mobile and Ubiquitous Computing, (<b>ACM MUM2021</b>) Leuven, Belgium, 2021. (Demo paper)</li>
                <li>Donghuo Zeng, Yi Yu, Keizo Oyama, "MusicTM-Dataset for Joint Representation Learning Among Sheet Music, Lyrics, and Musical Audio", Proceedings of the 8th Conference on Sound and Music Technology (<b>CSMT 2020</b>), 2021. (Full oral presentation)</li>
                <li>Donghuo Zeng, Yi Yu, Keizo Oyama, "Unsupervised generative adversarial alignment representation for sheet music, audio and lyrics", IEEE Sixth International Conference on Multimedia Big Data (<b>BigMM 2020</b>), 2020. (Short oral presentation) </li>
                <li>Donghuo Zeng, Yi Yu, Keizo Oyama, "Deep triplet neural networks with cluster-cca for audio-visual cross-modal retrieval", ACM Transactions on Multimedia Computing, Communications, and Applications, (<b>ACM TOMM 2020</b>) 2020. (IF: 3.144) </li>
                <li>Donghuo Zeng, Keizo Oyama, "Learning joint embedding for cross-modal retrieval", International Conference on Data Mining Workshops (<b>ICDMW 2019</b>), 2019. (Poster, oral presentation)</li>
                <li>Haoting Liang, Donghuo Zeng, Yi Yu, Keizo Oyama, "Personalized Music Recommendation with Triplet Network", The 11th Forum on Data Engineering and Information Management (<b>DEIM 2019</b>), 2019. (Oral presentation)</li>
                <li>Donghuo Zeng, Yi Yu, Keizo Oyama, "Audio-visual embedding for cross-modal music video retrieval through supervised deep CCA", The 20th IEEE International Symposium on Multimedia (<b>ISM 2018</b>), 2018. (Full oral presentation, Acceptance rate: 23%)</li>
                <li>Yi Yu, Stephane Beuret, Donghuo Zeng, Keizo Oyama, "Deep learning of human perception in audio event classification", The 20th IEEE International Symposium on Multimedia (<b>ISM 2018</b>), 2018. (Poster)</li>
                <li>Donghuo Zeng, Chengjie Sun, Lei Lin, Bing Liu, "LSTM-CRF for drug-named entity recognition", <b>Entropy, 2017</b>. (IF:2.2) </li> 
                <li>Donghuo Zeng, Chengjie Sun, Lei Lin, Bing Liu, "Enlarging drug dictionary with semi-supervised learning for Drug Entity Recognition", 2016 IEEE International Conference on Bioinformatics and Biomedicine (<b>BIBM 2016</b>), 2016. (Poster) </li>
            </ol>
        </div>
        
    </div>
</body>
</html>
